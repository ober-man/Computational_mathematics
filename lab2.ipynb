{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c10451",
   "metadata": {},
   "source": [
    "# Lab work 2\n",
    "## Numeric solution of a linear equations system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ac6d0",
   "metadata": {},
   "source": [
    "## 1. Gauss elimination algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fb0b2",
   "metadata": {},
   "source": [
    "It is a simple algorithm:  \n",
    "a) firstly we exclude variable with using elementary lines transformations, making a upper triangular matrix  \n",
    "b) then we sequentially find all variable since the last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be41877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu1 =  44.672322469556505\n",
      "Embedded algorithm:  [ 2.  3. -1.]\n",
      "Gauss algorithm:  [ 2.  3. -1.]\n",
      "\n",
      "\n",
      "mu2 =  16.393731622284385\n",
      "Embedded algorithm:  [1. 1. 1.]\n",
      "Gauss algorithm:  [0.55511151 0.25       0.25      ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "def dummy_Gauss(A, b):\n",
    "    n = b.size\n",
    "    \n",
    "    # forward elimination\n",
    "    for j in range(0, n-1):\n",
    "        for i in range(j+1, n):\n",
    "            if A[j,j] != 0:\n",
    "                num = A[i,j] / A[j,j]\n",
    "                A[i,j:n] -= num*A[j,j:n]\n",
    "                b[i] -= num*b[j]\n",
    "            \n",
    "    # back substitution\n",
    "    res = np.zeros(n) \n",
    "    for i in range(n-1, -1, -1):\n",
    "        if A[i,i] != 0:\n",
    "            res[i] = (b[i] - np.dot(A[i,i+1:n], res[i+1:n])) / A[i,i]\n",
    "    return res\n",
    "    \n",
    "# test\n",
    "A1 = np.array([[-3., -1.,  2.], \n",
    "              [ 2.,  1., -1.], \n",
    "              [-2.,  1.,  2.]])\n",
    "b1 = np.array([-11.,8.,-3.])\n",
    "print('mu1 = ', la.cond(A1))\n",
    "print(\"Embedded algorithm: \", la.solve(A1, b1))\n",
    "print(\"Gauss algorithm: \", dummy_Gauss(A1, b1))\n",
    "print(\"\\n\")\n",
    "\n",
    "# this system will be solved incorrectly\n",
    "# because of dividing by a big number\n",
    "A2 = np.array([[1e-16,  1., -1.], \n",
    "               [  -1.,  2., -1.], \n",
    "               [   2., -1.,  0.]]);\n",
    "b2 = np.array([0., 0., 1.])\n",
    "print('mu2 = ', la.cond(A2))\n",
    "print(\"Embedded algorithm: \", la.solve(A2, b2))\n",
    "print(\"Gauss algorithm: \", dummy_Gauss(A2, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041c8b1",
   "metadata": {},
   "source": [
    "That algorithm was bad because of big inaccuracy, appearing when the diagonal element is sufficient smaller than others.\n",
    "\n",
    "The complexity is $O\\left(\\frac{2}{3}n^3\\right)$\n",
    "\n",
    "There was used the conditional number counting as $\\mu = \\left\\|A^{-1}\\right\\| \\cdot \\|A\\| $ that measuring how much results can change because of small difference in input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e991f",
   "metadata": {},
   "source": [
    "## 1.1 Gauss elimination algorithm with the selection of the main element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321d47a",
   "metadata": {},
   "source": [
    "Let's improve Gauss algorithm. It had a big inaccuracy, when the diagonal element is sufficient smaller than others.  \n",
    "Now let's change lines to find the maximal element and divide by it.  \n",
    "It will reduce inaccuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a697b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu1 =  16.393731622284385\n",
      "Gauss algorithm:  [1. 1. 1.]\n",
      "Embedded algorithm:  [1. 1. 1.]\n",
      "\n",
      "\n",
      "mu2 =  39205.999974425096\n",
      "Gauss algorithm:  [1. 1.]\n",
      "Gauss algorithm for changed:  [ 2.97 -0.99]\n",
      "\n",
      "\n",
      "mu3 =  2.453817034094917e+16\n",
      "LinAlgError exception: Singular matrix\n"
     ]
    }
   ],
   "source": [
    "def smart_Gauss(A, b):\n",
    "    n = b.size\n",
    "    \n",
    "    # forward elimination\n",
    "    for j in range(0, n-1):\n",
    "        for i in range(j+1, n):\n",
    "            \n",
    "            #find a max element in j column\n",
    "            max = abs(A[j,j])\n",
    "            str = j\n",
    "            for k in range(j+1, n):\n",
    "                if abs(A[k,j]) > max:\n",
    "                    max = abs(A[k,j])\n",
    "                    str = k\n",
    "            \n",
    "            # if max element is 0, matrix is singular\n",
    "            if max == 0:\n",
    "                raise la.LinAlgError(\"Singular matrix\")\n",
    "                \n",
    "            # else change rows' positions\n",
    "            else:\n",
    "                for k in range(n):\n",
    "                    tmp = A[j,k]\n",
    "                    A[j,k] = A[str,k]\n",
    "                    A[str,k] = tmp\n",
    "            b[j], b[str] = b[str], b[j]\n",
    "            \n",
    "            num = A[i,j] / A[j,j]\n",
    "            A[i,j:n] -= num*A[j,j:n]\n",
    "            b[i] -= num*b[j]\n",
    "                \n",
    "    # back substitution\n",
    "    res = np.zeros(n)\n",
    "    # necessary and sufficient conditions of convergence of Gauss method\n",
    "    for i in range(n-1, -1, -1):\n",
    "        if A[i,i] != 0:\n",
    "            res[i] = (b[i] - np.dot(A[i,i+1:n], res[i+1:n])) / A[i,i]\n",
    "        else:\n",
    "            raise la.LinAlgError(\"Singular matrix\")\n",
    "    return res\n",
    "\n",
    "# test\n",
    "try:\n",
    "    A1 = np.array([[1e-16,  1., -1.], \n",
    "                   [  -1.,  2., -1.], \n",
    "                   [   2., -1.,  0.]]);\n",
    "    b1 = np.array([0., 0., 1.])\n",
    "    print('mu1 = ', la.cond(A1))\n",
    "    print(\"Gauss algorithm: \", smart_Gauss(A1, b1))\n",
    "    print(\"Embedded algorithm: \", la.solve(A1, b1))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "except la.LinAlgError as lae:\n",
    "    print(\"LinAlgError exception: \" + str(lae))\n",
    "    \n",
    "try:\n",
    "    A2 = np.array([[100.,99.], \n",
    "                   [ 99.,98.]]);\n",
    "    b2 = np.array([199., 197.])\n",
    "    A21 = np.array([[100.,99.], \n",
    "                    [ 99.,98.]]);\n",
    "    b21 = np.array([198.99, 197.01])\n",
    "    print('mu2 = ', la.cond(A2))\n",
    "    print(\"Gauss algorithm: \", smart_Gauss(A2, b2))\n",
    "    print(\"Gauss algorithm for changed: \", smart_Gauss(A21, b21))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "except la.LinAlgError as lae:\n",
    "    print(\"LinAlgError exception: \" + str(lae))\n",
    "\n",
    "try:\n",
    "    A3 = np.array([[1., 2., 3.], \n",
    "                   [2., 4., 6.], \n",
    "                   [0., 1., 2.]])\n",
    "    b3 = np.array([8., -11., -3.])\n",
    "    print('mu3 = ', la.cond(A3))\n",
    "    # there will be an exception\n",
    "    print(\"Gauss algorithm: \", smart_Gauss(A3, b3))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "except la.LinAlgError as lae:\n",
    "    print(\"LinAlgError exception: \" + str(lae))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf6fc5",
   "metadata": {},
   "source": [
    "In example 1, how one can see, this algorithm is better - it gives correct answer.  \n",
    "Example 2 shows, how small difference (about 0.01) can change the result if the matrix has bad condition number: \n",
    "$\\mu \\approx 4\\cdot10^4$  \n",
    "And the example 3 shows that system with singular matrix cannot be solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f3c82",
   "metadata": {},
   "source": [
    "## 2. Tridiagonal matrix algorithm or Thomas algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c985e",
   "metadata": {},
   "source": [
    "This algorithm solve a system $ Ax = b $ with 3-diagonal matrix:\n",
    "$$\n",
    "A = \\begin{pmatrix} \n",
    "\\beta_1 & \\gamma_1 \\\\\n",
    "\\alpha_2 & \\beta_2 & \\gamma_2 \\\\\n",
    "& \\ddots & \\ddots & \\ddots \\\\\n",
    "& & \\alpha_{n-1} & \\beta_{n-1} & \\gamma_{n-1} \\\\\n",
    "& & & \\alpha_n & \\beta_n \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Assuming $$ x_i = P_i x_{i+1} + Q_i, ~~ x_n = Q_n$$\n",
    "where coefficients can be calculated as \n",
    "$$ P_i = \\frac{\\gamma_i}{-\\beta_i - \\alpha_i P_{i-1}} \\\\\n",
    "   Q_i = \\frac{\\alpha_i Q_{i-1} - b_i}{-\\beta_i - \\alpha_i P_{i-1}} \\\\\n",
    "   P_1 = \\frac{\\gamma_1}{-\\beta_1}, ~~ Q_1 = \\frac{b_1}{\\beta_1}, ~~ P_n = 0 $$\n",
    "   \n",
    "The complexity is $ O(n) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b70206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded algorithm:  [1. 1. 1. 1.]\n",
      "diagonally dominant\n",
      "Tridiagonal algorithm:  [1. 1. 1. 1.]\n",
      "\n",
      "\n",
      "Embedded algorithm:  [1. 2. 3. 4.]\n",
      "no diagonally dominant\n",
      "Tridiagonal algorithm:  [1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "def checkDominance(a):\n",
    "    n = len(a)\n",
    "    \n",
    "    # sufficient condition: diagonally dominant matrix\n",
    "    if abs(a[0,0]) < abs(a[0,1]):\n",
    "        print(\"no diagonally dominant\")\n",
    "        return\n",
    "    for i in range(1, n-1):\n",
    "        if abs(a[i,i]) < abs(a[i,i-1]) + abs(a[i,i+1]):\n",
    "            print(\"no diagonally dominant\")\n",
    "            return\n",
    "    if abs(a[n-1,n-1]) < abs(a[n-1,n-2]):\n",
    "        print(\"no diagonally dominant\")\n",
    "        return\n",
    "    \n",
    "    print(\"diagonally dominant\")\n",
    "    return\n",
    "\n",
    "def tridiag(A, b):\n",
    "    checkDominance(A)\n",
    "        \n",
    "    n = b.size\n",
    "    alpha = np.diagonal(A, offset = -1)\n",
    "    beta = np.diagonal(A)\n",
    "    gamma = np.diagonal(A, offset = 1)\n",
    "    \n",
    "    P = np.zeros(n)\n",
    "    P[0] = -gamma[0] / beta[0]\n",
    "    for i in range(1, n-1):\n",
    "        P[i] = gamma[i] / (-beta[i] - alpha[i-1]*P[i-1])\n",
    "    P[n-1] = 0\n",
    "    \n",
    "    Q = np.zeros(n)\n",
    "    Q[0] = b[0] / beta[0]\n",
    "    for i in range(1, n):\n",
    "        Q[i] = (alpha[i-1]*Q[i-1] - b[i]) / (-beta[i] - alpha[i-1]*P[i-1])\n",
    "        \n",
    "    x = np.zeros(n)\n",
    "    x[n-1] = Q[n-1]\n",
    "    for i in range(n-2, -1, -1):\n",
    "        x[i] = P[i]*x[i+1] + Q[i]\n",
    "    \n",
    "    return x\n",
    "\n",
    "# test\n",
    "A1 = np.array([[5., 3., 0., 0.], \n",
    "               [3., 6., 1., 0.], \n",
    "               [0., 1., 4., -2.],\n",
    "               [0., 0., 1., -3.]]);\n",
    "b1 = np.array([8., 10., 3., -2.])\n",
    "print(\"Embedded algorithm: \", la.solve(A1, b1))\n",
    "print(\"Tridiagonal algorithm: \", tridiag(A1, b1))\n",
    "print(\"\\n\")\n",
    "\n",
    "A2 = np.array([[1.,  2.,  0., 0.], \n",
    "               [2., -1.,  1., 0.], \n",
    "               [0.,  1., -1., 1.],\n",
    "               [0.,  0.,  1., 1.]]);\n",
    "b2 = np.array([5., 3., 3., 7.])\n",
    "print(\"Embedded algorithm: \", la.solve(A2, b2))\n",
    "print(\"Tridiagonal algorithm: \", tridiag(A2, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e79350",
   "metadata": {},
   "source": [
    "In example 1, how one can see, we have diagonally dominant matrix, and everything is good.\n",
    "\n",
    "Example 2 shows that system with matrix within diagonally dominant can also be solved with using this algorithm.\n",
    "It means that sufficent condition isn't necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea4701",
   "metadata": {},
   "source": [
    "# Now let's consider iterative methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7af1f",
   "metadata": {},
   "source": [
    "## 3. Jacobi method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b967a",
   "metadata": {},
   "source": [
    "Let's decompose matrix $ A = L + D + U $\n",
    "to diagonal component D, a lower triangular part L and an upper triangular part U  \n",
    "Then make an iterative procedure $ x_{k+1} = -D^{-1}(L + U) x_k + D^{-1} b = B x_k + F $  \n",
    "\n",
    "It will converge, if:  \n",
    "a) matrix A is diagonally dominant: $|a_{ii}| > \\sum\\limits_{j \\ne i}^{n}|a_{ij}| $  \n",
    "b) $ \\|B\\| \\le q < 1 $  \n",
    "с) $ |\\lambda| < 1 $ for the roots of the equation $ det(L + \\lambda D + U) = 0 $  \n",
    "\n",
    "To get accuracy = $ \\varepsilon $, you should make iterations while $ \\|x_{k+1} - x_{k}\\| \\le \\frac{1-q}{q}\\varepsilon $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b21f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded algorithm:  [1. 2. 3.]\n",
      "diagonally dominant\n",
      "Jacobi algorithm:  [1.00000041 1.99999988 3.00000041]\n"
     ]
    }
   ],
   "source": [
    "def Jacobi(A, b, eps):\n",
    "    checkDominance(A)\n",
    "    n = b.size\n",
    "    L = np.zeros((n,n))\n",
    "    D = np.zeros((n,n))\n",
    "    U = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            L[i,j] = A[i,j]\n",
    "        D[i,i] = A[i,i]\n",
    "        for j in range(i+1, n):\n",
    "            U[i,j] = A[i,j]\n",
    "    \n",
    "    B = -la.inv(D).dot((L + U))\n",
    "    f = la.inv(D).dot(b)\n",
    "    x = np.zeros(n)\n",
    "    x_new = np.zeros(n)\n",
    "    q = la.norm(B, 1)\n",
    "    if q >= 1:\n",
    "        print(\"not converge\")\n",
    "        return x\n",
    "    \n",
    "    while True:\n",
    "        x_new = B.dot(x) + f\n",
    "        if la.norm(x_new - x, 2) <= (1-q)/q*eps:\n",
    "            break\n",
    "        x = x_new\n",
    "        \n",
    "    return x_new\n",
    "    \n",
    "    \n",
    "# test\n",
    "A1 = np.array([[ 1., -0.3,  0.2], \n",
    "               [0.3,  -1., -0.2], \n",
    "               [0.2, -0.3,   1.]])\n",
    "b1 = np.array([1., -2.3, 2.6])\n",
    "print(\"Embedded algorithm: \", la.solve(A1, b1))\n",
    "print(\"Jacobi algorithm: \", Jacobi(A1, b1, 1e-5))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5520bca",
   "metadata": {},
   "source": [
    "## 4. Gauss-Seidel method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3714a1e8",
   "metadata": {},
   "source": [
    "This method is similar to Jacobi method. We decompose $ A = L + D + U $ in the same way,  \n",
    "but iterative procedure differs:  $ x_{k+1} = -(L + D)^{-1}U x_k + (L + D)^{-1} b = B x_k + F $  \n",
    "\n",
    "It will converge, if:  \n",
    "a) matrix A is diagonally dominant: $|a_{ii}| > \\sum\\limits_{j \\ne i}^{n}|a_{ij}| $  \n",
    "b) $ \\|B\\| \\le q < 1 $  \n",
    "с) $ |\\lambda| < 1 $ for the roots of the equation $ det(\\lambda L + \\lambda D + U) = 0 $  \n",
    "\n",
    "To get accuracy = $ \\varepsilon $, you should make iterations while $ \\|x_{k+1} - x_{k}\\| \\le \\frac{1-q}{q}\\varepsilon $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c1d20dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded algorithm:  [1. 2. 3.]\n",
      "diagonally dominant\n",
      "Gauss-Seidel algorithm:  [1.00000065 2.00000025 2.99999994]\n"
     ]
    }
   ],
   "source": [
    "def GaussSeidel(A, b, eps):\n",
    "    checkDominance(A)\n",
    "    n = b.size\n",
    "    L = np.zeros((n,n))\n",
    "    D = np.zeros((n,n))\n",
    "    U = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            L[i,j] = A[i,j]\n",
    "        D[i,i] = A[i,i]\n",
    "        for j in range(i+1, n):\n",
    "            U[i,j] = A[i,j]\n",
    "    \n",
    "    B = -la.inv(L + D).dot(U)\n",
    "    f = la.inv(L + D).dot(b)\n",
    "    x = np.zeros(n)\n",
    "    x_new = np.zeros(n)\n",
    "    q = la.norm(B, 1)\n",
    "    if q >= 1:\n",
    "        print(\"not converge\")\n",
    "        return x\n",
    "    \n",
    "    while True:\n",
    "        x_new = B.dot(x) + f\n",
    "        if la.norm(x_new - x, 2) <= (1-q)/q*eps:\n",
    "            break\n",
    "        x = x_new\n",
    "        \n",
    "    return x_new\n",
    "\n",
    "# test\n",
    "A1 = np.array([[ 1., -0.3,  0.2], \n",
    "               [0.3,  -1., -0.2], \n",
    "               [0.2, -0.3,   1.]])\n",
    "b1 = np.array([1., -2.3, 2.6])\n",
    "print(\"Embedded algorithm: \", la.solve(A1, b1))\n",
    "print(\"Gauss-Seidel algorithm: \", GaussSeidel(A1, b1, 1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3aa874",
   "metadata": {},
   "source": [
    "## 5.  Gradient descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043f780",
   "metadata": {},
   "source": [
    "As usual, we will solve a system $ Ax = b $  \n",
    "Let $ Ф(x) $ be a quadratic functional: $ Ф(x) = (Ax, x) - 2(b, x) + c $,  \n",
    "when $ (.,.) $ is a dot product of two vectors in a Hilbert space  \n",
    "One can note that a gradient of this functional gives us our linear system: $ \\nabla Ф = 2(Ax - b) $  \n",
    "Then solution of our system coincide with a vector making $ Ф = min $  \n",
    "\n",
    "Let's make an iterative process $ x_{k+1} = x_k - \\tau_k r_k $,  \n",
    "when $ r_k $ is known as residual: $ r_k = Ax_k - b $  \n",
    "and $ \\tau_k = \\frac{(r_k, r_k)}{(Ar_k, r_k)} $  \n",
    "\n",
    "Process can be ended, when $ \\|r_k\\| < \\varepsilon $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30deaad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded algorithm:  [1. 0.]\n",
      "Gradient descent algorithm:  [9.9999232e-01 5.1200000e-06]\n"
     ]
    }
   ],
   "source": [
    "def gradDesc(A, b, eps):\n",
    "    n = b.size\n",
    "    x = np.zeros(n)\n",
    "    counter = 0\n",
    "    while True:\n",
    "        if counter > 1000:\n",
    "            print(\"the method diverges\")\n",
    "            return x\n",
    "        r = A.dot(x) - b\n",
    "        if la.norm(r, 2) < eps:\n",
    "            break\n",
    "        tau = r.dot(r) / A.dot(r).dot(r)\n",
    "        x = x - tau*r\n",
    "        counter += 1\n",
    "    return x\n",
    "\n",
    "# test\n",
    "A1 = np.array([[1., 1.],  \n",
    "               [1., 2.]])\n",
    "b1 = np.array([1., 1])\n",
    "print(\"Embedded algorithm: \", la.solve(A1, b1))\n",
    "print(\"Gradient descent algorithm: \", gradDesc(A1, b1, 1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f9206",
   "metadata": {},
   "source": [
    "## 6. Minimal residual method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f765bf",
   "metadata": {},
   "source": [
    "This method is analogical to the previous.  \n",
    "But now let's define $ \\tau_k $ as $ \\tau_k = \\frac{(Ar_k, r_k)}{(Ar_k, Ar_k)} $  \n",
    "This statement was taken from the condition of minimal norm of the residual: $ \\|r_k\\| = min $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21caf53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded algorithm:  [1. 0.]\n",
      "Minimal residual algorithm:  [ 9.99997812e-01 -6.85215773e-17]\n"
     ]
    }
   ],
   "source": [
    "def minRes(A, b, eps):\n",
    "    n = b.size\n",
    "    x = np.zeros(n)\n",
    "    counter = 0\n",
    "    while True:\n",
    "        if counter > 1000:\n",
    "            print(\"the method diverges\")\n",
    "            return x\n",
    "        r = A.dot(x) - b\n",
    "        if la.norm(r, 2) < eps:\n",
    "            break\n",
    "        tau = A.dot(r).dot(r) / A.dot(r).dot(A.dot(r))\n",
    "        x = x - tau*r\n",
    "        counter += 1\n",
    "    return x\n",
    "\n",
    "# test\n",
    "A1 = np.array([[1., 1.],  \n",
    "               [1., 2.]])\n",
    "b1 = np.array([1., 1])\n",
    "print(\"Embedded algorithm: \", la.solve(A1, b1))\n",
    "print(\"Minimal residual algorithm: \", minRes(A1, b1, 1e-5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
